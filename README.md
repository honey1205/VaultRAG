# ğŸ“š VaultRAG

**VaultRAG** is a powerful, privacy-focused Retrieval Augmented Generation (RAG) system designed to run entirely offline. It enables secure and intelligent question-answering over your own documents using open-source Large Language Models (LLMs)â€”with zero data sent to third-party services.

---

## ğŸš€ Key Features

âœ… **Fully Offline & Private**  
Run LLMs and generate embeddings entirely on your local machine. No external API calls, no cloud, no compromises on data privacy.

âœ… **Multi-Source Document Ingestion**  
Seamlessly ingest content from a variety of sources:
- ğŸ“„ Local Files (PDF, TXT, etc.)
- ğŸ§  GitHub Repositories
- ğŸŒ Websites

âœ… **Streaming LLM Responses**  
Get real-time answers with streamed outputs for a smoother, faster experience.

âœ… **Conversational Memory**  
Enjoy a natural, ongoing conversation with context-aware responses.

âœ… **Chat Export**  
Save and revisit your Q&A sessions anytime.

---

## ğŸ”’ Why VaultRAG?

VaultRAG is built for developers, researchers, and organizations that care deeply about:
- Data Sovereignty
- Offline Accessibility
- Open-Source Transparency

Perfect for use in research labs, regulated industries, or air-gapped environments.

---

## ğŸ“¦ Coming Soon / Roadmap

- ğŸ” Advanced Search & Filters  
- ğŸ§© Plugin System  
- ğŸ“Š Usage Analytics (local-only)  
- ğŸ“± PWA Support  

---

## ğŸ› ï¸ Getting Started

```bash
# Clone the repo
git clone https://github.com/honey1205/VaultRAG.git
cd vaultrag

# Set up the environment and install dependencies
pip install -r requirements.txt

# Run the app
streamlit run main.py
