
# ğŸ§  VaultRAG

> **Private. Offline. Open-Source. Secure.**  
> Local Retrieval-Augmented Generation (RAG) system using open-source LLMsâ€”without sending your data to the cloud.

![Demo](demo.gif)

---

## âœ¨ Features

- ğŸ”’ **Fully Offline**: No API keys, no external callsâ€”your data stays private.
- ğŸ“„ **Multi-format File Support**: Ingest PDFs, DOCX, and text files effortlessly.
- ğŸ§© **Modular Architecture**: Customize loaders, chunkers, vector stores, and LLMs.
- ğŸ§  **LLM Flexibility**: Works with Ollama, LLaMA, Mistral, and other local models.
- âš¡ **Fast Retrieval**: Uses vector embeddings and similarity search for quick responses.
- ğŸŒ **Streamlit UI**: Simple, intuitive frontend for interacting with your local knowledge base.

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/honey1205/VaultRAG.git
cd VaultRAG
```

### 2. Install Requirements

```bash
pip install -r requirements.txt
```

> âš ï¸ Make sure you're using a Python 3.10+ environment.

### 3. Run the App

```bash
streamlit run app.py
```

---

## ğŸ“ Folder Structure

```bash
VaultRAG/
â”œâ”€â”€ app.py               # Streamlit app
â”œâ”€â”€ ingest.py            # File loader and vector store builder
â”œâ”€â”€ query.py             # Handles RAG logic
â”œâ”€â”€ utils/               # Utility functions
â”œâ”€â”€ data/                # Your source files go here
â”œâ”€â”€ vectorstore/         # Persisted embeddings
â””â”€â”€ requirements.txt     # Python dependencies
```

---

## ğŸ”§ Configuration

You can modify model behavior, chunk size, or retriever type in `config.py`. Default setup uses `llama_index` with `Ollama`.

---

## ğŸ“Œ Example Models

Use with local models like:

- `llama2`
- `mistral`
- `gemma`
- `codellama`

```bash
ollama run mistral
```
